{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from meteostat import Hourly\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time period\n",
    "start = datetime(1973, 1, 1)\n",
    "end = datetime(2023, 10, 15)\n",
    "# Get hourly data\n",
    "data = Hourly('72219', start, end) \n",
    "data = data.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+-----+----+----+------+----+----+\n",
      "|temp|dwpt|rhum|prcp|snow| wdir|wspd|wpgt|  pres|tsun|coco|\n",
      "+----+----+----+----+----+-----+----+----+------+----+----+\n",
      "|15.6|15.0|96.0| 0.0| NaN|180.0| 9.4| NaN|1017.6| NaN| NaN|\n",
      "|15.6|15.0|96.0| 0.0| NaN|270.0| 7.6| NaN|1018.7| NaN| NaN|\n",
      "|12.8|11.7|93.0| 0.8| NaN|350.0|14.8| NaN|1019.5| NaN| NaN|\n",
      "|12.2|11.1|93.0| 0.3| NaN|350.0| 9.4| NaN|1019.5| NaN| NaN|\n",
      "|12.8|11.0|89.0| 0.0| NaN|350.0|13.0| NaN|1019.7| NaN| NaN|\n",
      "|12.2|10.6|90.0| 0.0| NaN|  NaN| 0.0| NaN|1020.1| NaN| NaN|\n",
      "|12.2|10.6|90.0| 0.0| NaN|330.0|14.8| NaN|1020.1| NaN| NaN|\n",
      "|12.2| 9.4|83.0| 0.0| NaN|330.0|11.2| NaN|1020.0| NaN| NaN|\n",
      "|11.7| 7.2|74.0| 0.0| NaN|330.0|14.8| NaN|1020.0| NaN| NaN|\n",
      "|11.1| 6.6|74.0| 0.0| NaN|340.0|13.0| NaN|1020.0| NaN| NaN|\n",
      "|10.6| 4.9|68.0| 0.0| NaN|340.0|14.8| NaN|1019.7| NaN| NaN|\n",
      "|10.0| 4.4|68.0| 0.0| NaN|360.0|18.7| NaN|1019.1| NaN| NaN|\n",
      "| 8.3| 3.3|71.0| 0.0| NaN|340.0|13.0| NaN|1020.4| NaN| NaN|\n",
      "| 7.8| 1.6|65.0| 0.0| NaN|320.0|14.8| NaN|1021.0| NaN| NaN|\n",
      "| 6.7| 1.2|68.0| 0.0| NaN|340.0|18.7| NaN|1021.9| NaN| NaN|\n",
      "| 7.8| 1.6|65.0| 0.0| NaN|340.0|16.6| NaN|1022.9| NaN| NaN|\n",
      "| 9.4| 1.8|59.0| 0.0| NaN|330.0|16.6| NaN|1023.2| NaN| NaN|\n",
      "|11.7| 1.0|48.0| 0.0| NaN|330.0|11.2| NaN|1021.9| NaN| NaN|\n",
      "|13.3| 0.6|42.0| 0.0| NaN|320.0|13.0| NaN|1021.2| NaN| NaN|\n",
      "|15.0|-1.2|33.0| 0.0| NaN|320.0|14.8| NaN|1020.2| NaN| NaN|\n",
      "+----+----+----+----+----+-----+----+----+------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temp: double (nullable = true)\n",
      " |-- dwpt: double (nullable = true)\n",
      " |-- rhum: double (nullable = true)\n",
      " |-- prcp: double (nullable = true)\n",
      " |-- snow: double (nullable = true)\n",
      " |-- wdir: double (nullable = true)\n",
      " |-- wspd: double (nullable = true)\n",
      " |-- wpgt: double (nullable = true)\n",
      " |-- pres: double (nullable = true)\n",
      " |-- tsun: double (nullable = true)\n",
      " |-- coco: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df2 = df.withColumn(\"Date\", to_timestamp('Date', 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = df2.select(\"Date\").rdd.flatMap(lambda x: x).collect()\n",
    "# y = df2.select(\"Primar_yType\").group .rdd.flatMap(lambda x: x).collect()\n",
    "# plt.scatter(x, y)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library\n",
    "from meteostat import Stations\n",
    "\n",
    "# Get nearby weather stations\n",
    "stations = Stations()\n",
    "stations = stations.nearby(34.8344640, 10.7697791)\n",
    "\n",
    "# Print DataFrame\n",
    "dff = stations.fetch(5000)\n",
    "places = dff[dff.country  ==\"TN\"].loc[:,['latitude','longitude','name','country','region',\"elevation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "import requests\n",
    "import json\n",
    "lat = 34.8344640\n",
    "lon = 10.7697791\n",
    "# part = \"minutely,hourly,daily,alerts\"\n",
    "# api_url = f\"http://api.openweathermap.org/data/3.0/onecall?lat={lat}&lon={lon}&exclude={part}&appid=7b8272ddd05780cc2edbcefd63a4bd08\"\n",
    "api_url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/Tunisia?unitGroup=metric&key=4G8D92T4DW62MX8ACJS4KZEP7&contentType=json\"\n",
    "\n",
    "def send_request_to_api(api_url) -> json:\n",
    "  response = requests.get(api_url)\n",
    "  if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    return data\n",
    "  else:\n",
    "    return {f\"Error: {response.status_code}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = send_request_to_api(api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = send_request_to_api(api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 == d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+-----------+\n",
      "|country|humidity|pressure|temperature|\n",
      "+-------+--------+--------+-----------+\n",
      "|     TN|      37|    1022|     289.04|\n",
      "+-------+--------+--------+-----------+\n",
      "\n",
      "[WRITE_STREAM_NOT_ALLOWED] `writeStream` can be called only on streaming Dataset/DataFrame.\n",
      "+-------+--------+--------+-----------+\n",
      "|country|humidity|pressure|temperature|\n",
      "+-------+--------+--------+-----------+\n",
      "|     TN|      37|    1022|     289.04|\n",
      "+-------+--------+--------+-----------+\n",
      "\n",
      "[WRITE_STREAM_NOT_ALLOWED] `writeStream` can be called only on streaming Dataset/DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\Desktop\\Hamza Bouajila\\3IDSD SD\\Spark\\TP\\Projet\\WeatherForcast\\app\\research\\meteostat.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/Hamza%20Bouajila/3IDSD%20SD/Spark/TP/Projet/WeatherForcast/app/research/meteostat.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     data_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(data_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/Hamza%20Bouajila/3IDSD%20SD/Spark/TP/Projet/WeatherForcast/app/research/meteostat.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mjson(sc\u001b[39m.\u001b[39mparallelize([data_json]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/Hamza%20Bouajila/3IDSD%20SD/Spark/TP/Projet/WeatherForcast/app/research/meteostat.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     df\u001b[39m.\u001b[39;49mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/Hamza%20Bouajila/3IDSD%20SD/Spark/TP/Projet/WeatherForcast/app/research/meteostat.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     df\u001b[39m.\u001b[39mwriteStream\u001b[39m.\u001b[39moutputMode(\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mconsole\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstart()\u001b[39m.\u001b[39mawaitTermination()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/Hamza%20Bouajila/3IDSD%20SD/Spark/TP/Projet/WeatherForcast/app/research/meteostat.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "import time\n",
    "while True:\n",
    "    try:\n",
    "        data = send_request_to_api(api_url)\n",
    "        time.sleep(5)\n",
    "        data_dict = {\n",
    "             \"country\":data[\"sys\"][\"country\"],\n",
    "             \"temperature\":data[\"main\"][\"temp\"],\n",
    "             \"humidity\":data[\"main\"][\"humidity\"],\n",
    "             \"pressure\":data[\"main\"][\"pressure\"]\n",
    "             }\n",
    "        data_json = json.dumps(data_dict)\n",
    "        df = spark.read.json(sc.parallelize([data_json]))\n",
    "        df.show()\n",
    "        df.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stremalit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 23:22:14.074 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2023-11-16 23:22:14.079 `st.experimental_memo` is deprecated. Please use the new command `st.cache_data` instead, which has the same behavior. More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
      "2023-11-16 23:22:14.081 No runtime found, using MemoryCacheStorageManager\n",
      "2023-11-16 23:22:14.298 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "import time  # to simulate a real time data, time loop\n",
    "\n",
    "import numpy as np  # np mean, np random\n",
    "import pandas as pd  # read csv, df manipulation\n",
    "import plotly.express as px  # interactive charts\n",
    "import streamlit as st  # üéà data web app development\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Real-Time Data Science Dashboard\",\n",
    "    page_icon=\"‚úÖ\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "# read csv from a github repo\n",
    "dataset_url = \"https://raw.githubusercontent.com/Lexie88rus/bank-marketing-analysis/master/bank.csv\"\n",
    "\n",
    "# read csv from a URL\n",
    "@st.experimental_memo\n",
    "def get_data() -> pd.DataFrame:\n",
    "    return pd.read_csv(dataset_url)\n",
    "\n",
    "df = get_data()\n",
    "\n",
    "# dashboard title\n",
    "st.title(\"Real-Time / Live Data Science Dashboard\")\n",
    "\n",
    "# top-level filters\n",
    "job_filter = st.selectbox(\"Select the Job\", pd.unique(df[\"job\"]))\n",
    "\n",
    "# creating a single-element container\n",
    "placeholder = st.empty()\n",
    "\n",
    "# dataframe filter\n",
    "df = df[df[\"job\"] == job_filter]\n",
    "\n",
    "# near real-time / live feed simulation\n",
    "for seconds in range(200):\n",
    "\n",
    "    df[\"age_new\"] = df[\"age\"] * np.random.choice(range(1, 5))\n",
    "    df[\"balance_new\"] = df[\"balance\"] * np.random.choice(range(1, 5))\n",
    "\n",
    "    # creating KPIs\n",
    "    avg_age = np.mean(df[\"age_new\"])\n",
    "\n",
    "    count_married = int(\n",
    "        df[(df[\"marital\"] == \"married\")][\"marital\"].count()\n",
    "        + np.random.choice(range(1, 30))\n",
    "    )\n",
    "\n",
    "    balance = np.mean(df[\"balance_new\"])\n",
    "\n",
    "    with placeholder.container():\n",
    "\n",
    "        # create three columns\n",
    "        kpi1, kpi2, kpi3 = st.columns(3)\n",
    "\n",
    "        # fill in those three columns with respective metrics or KPIs\n",
    "        kpi1.metric(\n",
    "            label=\"Age ‚è≥\",\n",
    "            value=round(avg_age),\n",
    "            delta=round(avg_age) - 10,\n",
    "        )\n",
    "        \n",
    "        kpi2.metric(\n",
    "            label=\"Married Count üíç\",\n",
    "            value=int(count_married),\n",
    "            delta=-10 + count_married,\n",
    "        )\n",
    "        \n",
    "        kpi3.metric(\n",
    "            label=\"A/C Balance ÔºÑ\",\n",
    "            value=f\"$ {round(balance,2)} \",\n",
    "            delta=-round(balance / count_married) * 100,\n",
    "        )\n",
    "\n",
    "        # create two columns for charts\n",
    "        fig_col1, fig_col2 = st.columns(2)\n",
    "        with fig_col1:\n",
    "            st.markdown(\"### First Chart\")\n",
    "            fig = px.density_heatmap(\n",
    "                data_frame=df, y=\"age_new\", x=\"marital\"\n",
    "            )\n",
    "            st.write(fig)\n",
    "            \n",
    "        with fig_col2:\n",
    "            st.markdown(\"### Second Chart\")\n",
    "            fig2 = px.histogram(data_frame=df, x=\"age_new\")\n",
    "            st.write(fig2)\n",
    "\n",
    "        st.markdown(\"### Detailed Data View\")\n",
    "        st.dataframe(df)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'streamlit' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exÔøΩcutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!streamlit run c:\\Users\\LENOVO\\anaconda3\\envs\\spark\\Lib\\site-packages\\ipykernel_launcher.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
